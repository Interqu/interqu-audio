# Interqu-audio

The goal - analyze waveforms to recognize unwanted / wanted speed patterns in an interview. Something of this complexity has 2 clear routes as of now:

1. Convert audio into text form and feed to chatGPT and analyze using chatGPT
2. Directly perform analysis on waveform. - (personal preference)

## Resources
- [Torchaudio Documentation](https://pytorch.org/audio/stable/index.html)
- [Audio manipulation (torchaudio)](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)
- [MFCC Mel Frequency Cepstral Coefficient](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)
- [Real-Time Speech Emotion Recognition](https://www.frontiersin.org/articles/10.3389/fcomp.2020.00014/full)
- [Speech/emotion recognition GITHUB](https://github.com/SuyashMore/MevonAI-Speech-Emotion-Recognition)