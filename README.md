# Interqu-audio

## NOTES
- MODEL - Huggingface Speech2Text model
- Use 16khz sample rate audio

## Audio Sample Sources
- [16khz](http://www.fit.vutbr.cz/~motlicek/speech_hnm.html) - audio1, audio2, audio3
- [16khz Sound Demo for the Wu-Wang](https://web.cse.ohio-state.edu/~wang.77/pnl/demo/WuReverb.html)

## Resources
- [Torchaudio Documentation](https://pytorch.org/audio/stable/index.html)
- [Audio manipulation (torchaudio)](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)
- [MFCC Mel Frequency Cepstral Coefficient](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)
- [Real-Time Speech Emotion Recognition](https://www.frontiersin.org/articles/10.3389/fcomp.2020.00014/full)
- [Speech/emotion recognition GITHUB](https://github.com/SuyashMore/MevonAI-Speech-Emotion-Recognition)